#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒæ—¥å¿—å»é‡è„šæœ¬
å¯¹äºå®Œå…¨é‡å¤çš„å®éªŒæ—¥å¿—å†…å®¹ï¼Œåªä¿ç•™ä¸€ä¸ª
"""
import json
import os
import sys
import shutil
from datetime import datetime
from collections import OrderedDict

def normalize_json(obj):
    """
    è§„èŒƒåŒ–JSONå¯¹è±¡ï¼Œç¡®ä¿ç›¸åŒå†…å®¹çš„JSONå­—ç¬¦ä¸²ä¸€è‡´
    é€šè¿‡é‡æ–°åºåˆ—åŒ–å¹¶æ’åºé”®æ¥å®ç°
    """
    # ä½¿ç”¨sort_keys=Trueç¡®ä¿ç›¸åŒå†…å®¹çš„JSONå­—ç¬¦ä¸²ä¸€è‡´
    return json.dumps(obj, sort_keys=True, ensure_ascii=False)

def deduplicate_log(input_file, output_file=None, backup=True, dry_run=False):
    """
    å¯¹å®éªŒæ—¥å¿—è¿›è¡Œå»é‡
    
    Args:
        input_file: è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
        backup: æ˜¯å¦å¤‡ä»½åŸæ–‡ä»¶
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰
    
    Returns:
        (åŸå§‹æ•°é‡, å»é‡åæ•°é‡, é‡å¤æ•°é‡)
    """
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ—¥å¿—æ–‡ä»¶ {input_file}")
        return None, None, None
    
    print("="*80)
    print("å®éªŒæ—¥å¿—å»é‡å·¥å…·")
    print("="*80)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    
    if output_file is None:
        output_file = input_file
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file} (è¦†ç›–åŸæ–‡ä»¶)")
    else:
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    if dry_run:
        print("âš ï¸  è¯•è¿è¡Œæ¨¡å¼: ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶")
    print("="*80)
    
    # è¯»å–æ‰€æœ‰æ—¥å¿—æ¡ç›®
    print("\nğŸ“– æ­£åœ¨è¯»å–æ—¥å¿—æ–‡ä»¶...")
    seen_records = OrderedDict()  # ä½¿ç”¨OrderedDictä¿æŒé¡ºåº
    duplicate_count = 0
    line_number = 0
    duplicate_examples = []  # ä¿å­˜å‰å‡ ä¸ªé‡å¤çš„ä¾‹å­
    max_examples = 5  # æœ€å¤šæ˜¾ç¤º5ä¸ªé‡å¤ä¾‹å­
    json_errors = []  # ä¿å­˜JSONè§£æé”™è¯¯
    
    with open(input_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            line_number += 1
            
            try:
                # è§£æJSON
                data = json.loads(line)
                
                # è§„èŒƒåŒ–JSONå­—ç¬¦ä¸²ä½œä¸ºå»é‡key
                normalized = normalize_json(data)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if normalized not in seen_records:
                    seen_records[normalized] = (line_num, line, data)
                else:
                    duplicate_count += 1
                    # ä¿å­˜å‰å‡ ä¸ªé‡å¤ä¾‹å­
                    existing_line_num, existing_line, existing_data = seen_records[normalized]
                    if len(duplicate_examples) < max_examples:
                        duplicate_examples.append((line_num, existing_line_num))
                    
            except json.JSONDecodeError as e:
                # å¯¹äºæ— æ³•è§£æçš„è¡Œï¼Œä¹Ÿä¿ç•™
                if line not in seen_records:
                    seen_records[line] = (line_num, line, None)
                else:
                    duplicate_count += 1
                    if len(json_errors) < 3:
                        json_errors.append((line_num, str(e)))
    
    # æ˜¾ç¤ºå¤„ç†è¿›åº¦
    if line_number > 0:
        print(f"  âœ… å·²å¤„ç† {line_number} è¡Œ")
    
    original_count = line_number
    unique_count = len(seen_records)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸå§‹æ—¥å¿—æ¡ç›®æ•°: {original_count}")
    print(f"  å»é‡åæ¡ç›®æ•°:   {unique_count}")
    print(f"  é‡å¤æ¡ç›®æ•°:     {duplicate_count}")
    if original_count > 0:
        print(f"  å»é‡ç‡:         {duplicate_count/original_count*100:.2f}%")
    
    # æ˜¾ç¤ºé‡å¤ä¾‹å­
    if duplicate_examples:
        print(f"\nğŸ“‹ é‡å¤ç¤ºä¾‹ï¼ˆå‰{len(duplicate_examples)}ä¸ªï¼‰:")
        for dup_line, orig_line in duplicate_examples:
            print(f"  è¡Œ {dup_line} ä¸è¡Œ {orig_line} å®Œå…¨ç›¸åŒ")
    
    # æ˜¾ç¤ºJSONè§£æé”™è¯¯
    if json_errors:
        print(f"\nâš ï¸  JSONè§£æé”™è¯¯ï¼ˆå‰{len(json_errors)}ä¸ªï¼‰:")
        for line_num, error in json_errors:
            print(f"  è¡Œ {line_num}: {error}")
    
    if duplicate_count == 0:
        print("\nâœ… æœªå‘ç°é‡å¤æ¡ç›®ï¼Œæ— éœ€å»é‡")
        return original_count, unique_count, duplicate_count
    
    if dry_run:
        print("\nâš ï¸  è¯•è¿è¡Œæ¨¡å¼: æœªå®é™…ä¿®æ”¹æ–‡ä»¶")
        return original_count, unique_count, duplicate_count
    
    # å¤‡ä»½åŸæ–‡ä»¶
    if backup and output_file == input_file:
        backup_file = f"{input_file}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"\nğŸ’¾ æ­£åœ¨å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
        shutil.copy2(input_file, backup_file)
        print(f"âœ… å¤‡ä»½å®Œæˆ")
    
    # å†™å…¥å»é‡åçš„ç»“æœ
    print(f"\nğŸ’¾ æ­£åœ¨å†™å…¥å»é‡åçš„ç»“æœåˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        for normalized_key, (line_num, line, data) in seen_records.items():
            f.write(line + '\n')
    
    print(f"âœ… å»é‡å®Œæˆï¼")
    print(f"\nğŸ“ æ–‡ä»¶ä¿¡æ¯:")
    print(f"  åŸå§‹æ–‡ä»¶å¤§å°: {os.path.getsize(input_file if backup else input_file) / 1024 / 1024:.2f} MB")
    if os.path.exists(output_file):
        print(f"  è¾“å‡ºæ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")
    
    return original_count, unique_count, duplicate_count

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='å®éªŒæ—¥å¿—å»é‡å·¥å…· - å¯¹å®Œå…¨é‡å¤çš„å®éªŒæ—¥å¿—å†…å®¹ï¼Œåªä¿ç•™ä¸€ä¸ª',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰
  python deduplicate_experiment_log.py --dry-run
  
  # å»é‡å¹¶è¦†ç›–åŸæ–‡ä»¶ï¼ˆè‡ªåŠ¨å¤‡ä»½ï¼‰
  python deduplicate_experiment_log.py
  
  # å»é‡å¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶
  python deduplicate_experiment_log.py --output experiment_results_dedup.log
  
  # å»é‡ä½†ä¸å¤‡ä»½
  python deduplicate_experiment_log.py --no-backup
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        default='experiment_results.log',
        help='è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: experiment_results.logï¼‰'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: è¦†ç›–åŸæ–‡ä»¶ï¼‰'
    )
    
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='ä¸å¤‡ä»½åŸæ–‡ä»¶ï¼ˆé»˜è®¤ä¼šå¤‡ä»½ï¼‰'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='è¯•è¿è¡Œæ¨¡å¼ï¼šåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå»é‡
    original_count, unique_count, duplicate_count = deduplicate_log(
        input_file=args.input,
        output_file=args.output,
        backup=not args.no_backup,
        dry_run=args.dry_run
    )
    
    if original_count is None:
        sys.exit(1)
    
    print("\n" + "="*80)
    print("å»é‡å®Œæˆï¼")
    print("="*80)
    
    if duplicate_count > 0 and not args.dry_run:
        print(f"\nğŸ’¡ æç¤º: åŸæ–‡ä»¶å·²å¤‡ä»½ï¼Œå¦‚éœ€æ¢å¤å¯ä½¿ç”¨å¤‡ä»½æ–‡ä»¶")

if __name__ == "__main__":
    main()
